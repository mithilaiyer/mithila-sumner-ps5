---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1: Mithila Iyer (CNET ID: 12414493)
    - Partner 2: Sumner Perera (CNET ID: 12403312)
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* SP
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from bs4 import BeautifulSoup
import requests

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

# 1. Extracts and saves HTML code as a parseable object

```{python}
#Loading in the Enforcement Actions webpage
url = 'https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')
#We want to find the titles, dates, categories, and links. Lets look at how many there are of each (we actually should know this as the first page lists 20)

#Titles these are under <a> tags in the <h2> tag with class 'usa-card__heading'. First we find h2 tags
h2_tags = soup.find_all('h2', class_='usa-card__heading')

#Then we filter these tags to find the <a> tags and the text within that we need with a loop
titles = [
    a_tag.get_text(strip=True)  
    for h2 in h2_tags  
    for a_tag in h2.find_all('a', href=True) 
    if a_tag['href'].startswith('/')  
]

#Print the # of filtered titles
print(len(titles))
#We have 20 titles - this seems to be on the right track!

#Dates
div_tags = soup.find_all('div', class_='font-body-sm margin-top-1')

#Extracting the text we need from sub-tags with a loop
dates = [
    span.get_text(strip=True)  
    for div in div_tags  
    for span in div.find_all('span', class_='text-base-dark') 
]

#Printing # of filtered dates
print(len(dates))

#Categories 
ul_tags = soup.find_all('ul', class_='display-inline add-list-reset')
#Extracting the text we need from sub-tags with a loop
categories = [
    li.get_text(strip=True)
    for ul in ul_tags 
    for li in ul.find_all('li', class_='usa-tag')
]
#Printing categories for each instance
print(len(categories))

#Links
#Importing url joiner package and setting base url for complete links
from urllib.parse import urljoin
base_url = 'https://oig.hhs.gov'

h2_tags = soup.find_all('h2', class_='usa-card__heading')

#Extracting full links using a loop
full_links = [
    urljoin(base_url, a_tag['href']) 
    for h2 in h2_tags 
    for a_tag in h2.find_all('a', href=True) 
]

#Printing the number of full links found
print(len(full_links))

#Creating dataframe
scraped_data = {'Title': titles, 'Date': dates, 'Category': categories, 'Link': full_links}
scraped_data = pd.DataFrame(scraped_data)

#Printing the head
print(scraped_data.head())

#Source: ChatGPT for extracting all titles, links, etc using a loop, and for looking up how to include entire links using urljoin
```


### 2. Crawling (PARTNER 1)

```{python}
# URL of the first page
main = 'https://oig.hhs.gov/fraud/enforcement/'

# Send request to the main page
response = requests.get(main)
soup = BeautifulSoup(response.content, 'html.parser')

# Extracting all the complete links from the first page
full_links = [
    urljoin(main, a_tag['href'])
    for h2 in soup.find_all('h2', class_='usa-card__heading')
    for a_tag in h2.find_all('a', href=True)
]

# Initialize an empty list to store agency names
agencies = []

# Scrape agency names from the first page's links
for link in full_links:
    # Get content from each individual page link
    response = requests.get(link)
    soup = BeautifulSoup(response.content, 'html.parser')
    ul_tag = soup.find('ul', class_='usa-list')

    # Default agency name to "MISSING" if not found
    agency_name = "MISSING"

    if ul_tag:
        li_tags = ul_tag.find_all('li')

        # Extract the agency info from the second <li> tag (if available)
        if len(li_tags) > 1:
            agency_info = li_tags[1].get_text(strip=True)

            # If 'Agency:' is found in the agency_info, extract the agency name
            if 'Agency:' in agency_info:
                agency_name = agency_info.split("Agency:")[1].strip().strip('"')

    # Append the agency name (or "MISSING" if not found)
    agencies.append(agency_name)

#Print the number of agencies found
print(agencies)
#We see that some pages have agency info missing - that is okay, we are going to keep those in the agency column right now. However, let's clean up the 2nd entry before we add it to the scraped dataframe (we see that on the website, it adds the date before the agency name)
def clean_agencies(agencies):
    cleaned_list = []
    for agency in agencies:
        if ";" in agency:
            cleaned_list.append(agency.split(";", 1)[1].strip())
        else:
            cleaned_list.append(agency)
    return cleaned_list
cleaned_agencies = clean_agencies(agencies)
```
``` {python} 
#Adding agencies to the scraped data
scraped_data['Agency'] = cleaned_agencies

#Printing head
print(scraped_data.head())

#Source: ChatGPT for help determining how to find information on the second li tag on a page, for understanding how to store agency names in a list and why that is necessary.
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

1. Create a date checker function
    1.1 Ask user for a month and a day 
    1.2 Check user input to make sure that it's >= 2013; if not then ask user to re-enter 
2. Create a scraper 
    2.1 Create an empty soup object for all the content on the page 
    2.2 Extract date text for the dates on the page, checking along the way whether the date matches the condition. If it does, keep going to the next page but if it doesn't then stop scraping.
    2.3 Get the size of this date list and then pull the same number of objects from the soup objects for the title, category, and the link 
    2.4 For each link gathered, grab the agency 
3. Combine all the lists into a dataframe 

To build this, I will use a while loop and smaller for loops. 

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
#Create the block of asking the user for a date 
from datetime import datetime 

#Create the function that checks the user date 
def get_dates():
    while True: 
        user_says = input("Enter a date (MM-YY): ")

        try:
            user_date = datetime.strptime(user_says, "%m-%y")
            if datetime(2013,1,1) <= user_date <= datetime.now():
                return user_date
            else:
                print("Date must be after 01-2013 and before today.")
        except ValueError:
            print("Invalid date format. Please use MM-YY")

def scrape_agency(link):
    try: 
        response = requests.get(link)
        soup = BeautifulSoup(response.content, 'html.parser')
        ul_tag = soup.find('ul', class_='usa-list') 
    
        if ul_tag:
            li_tags = ul_tag.find_all('li')
            if len(li_tags) > 1:
                agency_info = li_tags[1].get_text(strip=True)
                if 'Agency:' in agency_info:
                    agency_name = agency_info.split("Agency:")[1].strip().strip('"')
                    return agency_name
        return ""
    except requests.exceptions.RequestException as e:
        print(f"Error scraping {link}: {e}")
        return ""

#Create the function that scrapes 
def scraper(user_date): 
    url = "https://oig.hhs.gov/fraud/enforcement/?page="
    page = 1 
    dates = []
    titles = []
    category = []
    links = []
    agency = []
    
    while True: 
        current_url = f"{url}{page}"
        response = requests.get(current_url)
        soup = BeautifulSoup(response.content, "html.parser") 

        #Find tags on the current page 
        span_tags = soup.find_all("span", class_="text-base-dark padding-right-105")
        h2_tags = soup.find_all('h2', class_='usa-card__heading')
        ul_tags = soup.find_all('ul', class_='display-inline add-list-reset')

        #If there are no more pages 
        if not span_tags and not h2_tags: 
            print(f"Stopping. No more data on page {page}.")
            break 

        #Scrape the dates 
        date_found = False
        for span in span_tags: 
            date_text = span.get_text(strip=True)
            format_date = datetime.strptime(date_text, "%B %d, %Y")

            if format_date >= user_date: 
                dates.append(format_date)
                date_found = True 
            else:
                print(f"Date is older, stopping.")
                break 

        #If no valid date is found 
        if not date_found: 
            break

        #Scrape the titles
        for h2 in h2_tags:
            for a_tag in h2.find_all('a', href=True):
                if a_tag["href"].startswith("/"): 
                    title_text = a_tag.get_text(strip=True)
                    titles.append(title_text)
                if len(titles) >= len(dates): 
                    break 
            if len(titles) >= len(dates): 
                break

        #Scrape the full links 
        for h2 in h2_tags:
            for a_tag in h2.find_all('a', href=True):
                full_link = urljoin(base_url, a_tag['href'])
                links.append(full_link)
                if len(links) >= len(dates): 
                    break 
            if len(links) >= len(dates): 
                break

        #Scrape the categories 
        for ul in ul_tags:
            for li in ul.find_all('li', class_='usa-tag'):
                category_text = li.get_text(strip=True)
                category.append(category_text)
                if len(category) >= len(dates): 
                    break 
            if len(category) >= len(dates): 
                break   

        #Now scrape agency info
        for link in links:
            agency_name = scrape_agency(link)
            agency.append(agency_name)
            if len(agency) >= len(links):
                break

        #Move to the next page with pause
        page += 1
        print(f"Moving to page {page}...")
        time.sleep(1)

    #Create a dataframe 
    scraped = {
        "Dates" : dates,
        "Titles": titles, 
        "Category": category, 
        "Links": links, 
        "Agency": agency
    }

    scraped_df = pd.DataFrame(scraped)
    print ("Scraping Complete.")
    return scraped_df

#Combine both functions into one single function 
def dynamic_scraper(): 
    user_date = get_dates() 
    all_scraped = scraper(user_date)

    #Export to CSV 
    formatted_date_name = user_date.strftime("%y-%m")   
    file_name = f"enforcement_actions_{formatted_date_name}.csv"  
    all_scraped.to_csv(file_name, index=False)
    
    return 

#Used ChatGPT to design and build this scraper, asking for troubleshooting and how to pull the same number of values from every subsequent soup object. 

```


* c. Test Partner's Code (PARTNER 1)

```{python}
#Get all the values since Jan 2023 (01-23)
dynamic_scraper()

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
import geopandas as gpd 
import pandas as pd

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}
import os
#Loading in shapefile from PSet4
filepath = "/Users/mithilaiyer/Documents/GitHub/psetdata/shp/gz_2010_us_860_00_500k.shp" 
pset4_geo = gpd.read_file(filepath)
#Loading in zip-level population data
path = "/Users/mithilaiyer/Documents/GitHub/pset5data/Pop_Data/DECENNIALDHC2020.P1-Data.csv"
zip_pop_data = pd.read_csv(path, low_memory=False) 
zip_pop_data.head()

#Pre-merge cleaning: Deleting first row (var names), ensuring the ZCTA5 column only has numeric values, 

zip_pop_data = zip_pop_data.drop(index=0).reset_index(drop=True) 
zip_pop_data['NAME'] = zip_pop_data['NAME'].str.replace(r'ZCTA5\s*', '', regex=True)
zip_pop_data['NAME'] = zip_pop_data['NAME'].str.lstrip()

#Renaming some vars because they have similar names in geo + pop dataframes
zip_pop_data.rename(columns={'NAME': 'zip_name'}, inplace=True)

#Merging
merged_data = pd.merge(pset4_geo, zip_pop_data, how='inner', left_on='ZCTA5', right_on='zip_name')
```

### 2. Conduct spatial join
```{python}
#Loading in district shapefile
filepath2 = "/Users/mithilaiyer/Documents/GitHub/pset5data/shp/geo_export_c69fb57e-ec8d-4c28-a5d7-89d59a407c76.shp" 
district_shp = gpd.read_file(filepath2)

#Conducting spatial join 
joined_gdf = gpd.sjoin(merged_data, district_shp, how="inner", predicate="intersects")
#Removing duplicates?
new_data = joined_gdf.drop_duplicates(subset='geometry')
new_data['P1_001N'] = pd.to_numeric(new_data['P1_001N'], errors='coerce')

#Aggregating by population 
aggregated = new_data.groupby('judicial_d')['P1_001N'].sum().reset_index()


```

### 3. Map the action ratio in each district
```{python}



```